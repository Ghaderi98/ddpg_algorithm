{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Define the environment\n",
        "class StockMarketEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.start_date = '2020-01-01'\n",
        "        self.end_date = '2022-01-01'\n",
        "        self.ticker = 'AAPL'\n",
        "        self.df = yf.download(self.ticker, start=self.start_date, end=self.end_date)\n",
        "        self.df = self.df.dropna()\n",
        "        self.df['Returns'] = self.df['Close'].pct_change()\n",
        "        self.df = self.df.iloc[1:]\n",
        "        self.observations = len(self.df)\n",
        "        self.state_shape = 1\n",
        "        self.action_shape = 1\n",
        "        self.state = np.array([[self.df['Returns'][0]]])\n",
        "\n",
        "    def step(self, action):\n",
        "        last_period_close = self.df['Close'][self.current_step-1]\n",
        "        today_close = self.df['Close'][self.current_step]\n",
        "        predicted_close = today_close*(1+action[0])\n",
        "        self.df.loc[self.df.index[self.current_step], 'predicted_close'] = predicted_close\n",
        "        signal = 0\n",
        "        if predicted_close > last_period_close:\n",
        "            signal = 1\n",
        "        elif predicted_close < last_period_close:\n",
        "            signal = -1\n",
        "        reward = signal * self.df['Returns'][self.current_step]\n",
        "\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.observations:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "            self.state = np.array([[self.df['Returns'][self.current_step]]])\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.state = np.array([[self.df['Returns'][0]]])\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Define the agent\n",
        "class DDPGAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.001\n",
        "        self.buffer_size = 100000\n",
        "        self.batch_size = 6400\n",
        "        self.replay_buffer = []\n",
        "        self.q_model = models.Sequential([\n",
        "            layers.Dense(64, activation='relu', input_shape=(env.state_shape + env.action_shape,)),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(1)\n",
        "        ])\n",
        "        self.policy_model = models.Sequential([\n",
        "            layers.Dense(64, activation='relu', input_shape=(env.state_shape,)),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(env.action_shape, activation='tanh')\n",
        "        ])\n",
        "        self.q_optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "        self.policy_optimizer = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    def ddpg(self, state, action, reward, next_state, done):\n",
        "        # Add the transition to the replay buffer\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        # Sample a batch of transitions from the replay buffer\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states = np.array([transition[0] for transition in batch])\n",
        "        actions = np.array([transition[1] for transition in batch])\n",
        "        rewards = np.array([transition[2] for transition in batch])\n",
        "        next_states = np.array([transition[3] for transition in batch])\n",
        "        dones = np.array([transition[4] for transition in batch])\n",
        "\n",
        "        # Update the Q-network\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.policy_model(next_states)\n",
        "            target_q_values = self.q_model(tf.concat([next_states, target_actions], axis=1))\n",
        "            target_y = rewards + self.gamma * (1 - dones) * target_q_values\n",
        "            current_q_values = self.q_model(tf.concat([states, actions], axis=1))\n",
        "            loss = tf.reduce_mean(tf.square(current_q_values - target_y))\n",
        "        grads = tape.gradient(loss, self.q_model.trainable_weights)\n",
        "        self.q_optimizer.apply_gradients(zip(grads, self.q_model.trainable_weights))\n",
        "\n",
        "        # Update the policy network\n",
        "        with tf.GradientTape() as tape:\n",
        "            new_actions = self.policy_model(states)\n",
        "            q_values = self.q_model(tf.concat([states, new_actions], axis=1))\n",
        "            policy_loss = -tf.reduce_mean(q_values)\n",
        "        grads = tape.gradient(policy_loss, self.policy_model.trainable_weights)\n",
        "        self.policy_optimizer.apply_gradients(zip(grads, self.policy_model.trainable_weights))\n",
        "\n",
        "        # Update the target networks\n",
        "        q_weights = self.q_model.get_weights()\n",
        "        target_q_weights = self.target_q_model.get_weights()\n",
        "        for i in range(len(q_weights)):\n",
        "            target_q_weights[i] = self.tau * q_weights[i] + (1 - self.tau) * target_q_weights[i]\n",
        "        self.target_q_model.set_weights(target_q_weights)\n",
        "\n",
        "        policy_weights = self.policy_model.get_weights()\n",
        "        target_policy_weights = self.target_policy_model.get_weights()\n",
        "        for i in range(len(policy_weights)):\n",
        "            target_policy_weights[i] = self.tau * policy_weights[i] + (1 - self.tau) * target_policy_weights[i]\n",
        "        self.target_policy_model.set_weights(target_policy_weights)\n",
        "\n",
        "    def train(self, episodes):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.policy_model.predict(state)[0]\n",
        "                action = np.clip(action, -1, 1)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.ddpg(state, action, reward, next_state, done)\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "            print(f\"Episode: {episode+1}, Reward: {episode_reward}\")\n",
        "\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "    def test(self):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = self.policy_model.predict(state)[0]\n",
        "            action = np.clip(action, -1, 1)\n",
        "            state, _, done, _ = self.env.step(action)\n",
        "        print(\"Testing complete!\")\n",
        "\n",
        "    def plot_results(self):\n",
        "        self.env.render()\n",
        "        plt.plot(self.env.df['Close'], label='Actual Close')\n",
        "        plt.plot(self.env.df['predicted_close'], label='Predicted Close')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        returns = self.env.df['Returns'].values\n",
        "        predicted_returns = self.env.df['predicted_close'].pct_change().values[1:]\n",
        "        win_rate = (np.sign(returns[1:]) == np.sign(predicted_returns)).mean()\n",
        "        maximum_drawdown = (1 - (self.env.df['Close'] / self.env.df['Close'].cummax())).max()\n",
        "        net_profit = (self.env.df['predicted_close'].pct_change() + 1).cumprod()[-1]\n",
        "        loss_rate = 1 - win_rate\n",
        "        return win_rate, maximum_drawdown, net_profit, loss_rate\n",
        "\n",
        "# Create the environment\n",
        "env = StockMarketEnv()\n",
        "\n",
        "# Create the agent\n",
        "agent = DDPGAgent(env)\n",
        "\n",
        "# Train the agent\n",
        "episode_count = 100\n",
        "agent.train(episode_count)\n",
        "\n",
        "# Test the agent\n",
        "agent.test()\n",
        "\n",
        "# Plot the results\n",
        "agent.plot_results()\n",
        "\n",
        "# Calculate metrics\n",
        "win_rate, maximum_drawdown, net_profit, loss_rate = agent.calculate_metrics()\n",
        "print(f\"Win Rate: {win_rate}\")\n",
        "print(f\"Maximum Drawdown: {maximum_drawdown}\")\n",
        "print(f\"Net Profit: {net_profit}\")\n",
        "print(f\"Loss Rate: {loss_rate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BqdJ4Qbo6Dm",
        "outputId": "f2ac3f29-c242-492e-bdce-bb3164b20cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "1/1 [==============================] - 0s 167ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 124ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 131ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 170ms/step\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n"
          ]
        }
      ]
    }
  ]
}